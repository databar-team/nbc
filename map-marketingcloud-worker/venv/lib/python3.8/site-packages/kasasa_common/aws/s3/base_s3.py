"""
This module provides two main classes.
One of them, :class:`.RealS3`, represents an S3 bucket.
Another class, :class:`.LocalS3`, is primarily meant to be used for testing.
It provides the same interface as :class:`.RealS3`
but uses local directory as a backend.

The recommended method to obtain these classes
is with :func:`.get_s3` function.
It will first check if given bucket name can be resolved
to a local directory path (either relative or absolute).
If so then it will return a :class:`.LocalS3` instance.
Otherwise it will look for real S3 bucket with given name,
and raise an exception if one is not found.

In addition this module provides :func:`get_secrets` helper function
which allows convenient access to ``secrets.json`` file stored on S3 bucket.
"""

from __future__ import annotations
import io
import os.path as op
from typing import Union, BinaryIO, Generator, TextIO


class BucketNotAvailable(ValueError):
    pass


class ResponseWrapper(io.IOBase):
    # This is the wrapper around urllib3.HTTPResponse
    # to work-around an issue shazow/urllib3#1305.
    #
    # Here we decouple HTTPResponse's "closed" status from ours.
    #
    # FIXME drop this wrapper after shazow/urllib3#1305 is fixed

    def __init__(self, resp):
        self._resp = resp

    def close(self):
        self._resp.close()
        super(ResponseWrapper, self).close()

    def readable(self):
        return True

    def read(self, amt=None):
        if self._resp.closed:
            return b''
        return self._resp.read(amt)

    def readinto(self, b):
        val = self.read(len(b))
        if not val:
            return 0
        b[:len(val)] = val
        return len(val)


class BaseS3Helper(object):
    """
    Base class providing common interface for both `LocalS3` and `RealS3`.

    This object represents a single bucket.
    """

    def __init__(self, bucket: str):
        self.bucket: str = bucket

    def list(self, prefix: str = None, only_after: str = None) -> Generator:
        """
        Iterate over objects in this bucket, yielding their keys.
        Objects are returned in arbitrary order.

        Can also be accessed by iterating over the bucket::

            bucket = get_s3(bucket_name)
            for key in bucket:
                bucket.download(key, mydir)

        Alternatively accepts optional ``prefix``
        to yield only those objects which are in the certain "directory".

        :param str prefix:
            if specified then will only list objects
            which start with the given prefix.
            Note that unlike core S3 behaviour,
            here prefix is treated as a directory -
            i.e. ``path/to`` prefix will include ``path/to/file.txt``
            but will *not* include ``path/tommy.txt``
            or ``path/tommy/file.txt``.

        :param str only_after:
            skip objects until the given key is encountered.
            Will yield objects starting with the *next* one
            after the given key.
        """
        raise NotImplementedError

    def list_prefix(self, prefix: str) -> Generator:
        raise NotImplementedError

    def list_dirs(self, prefix: str = None):
        """
        While :meth:`list` will yield only keys (?),
        this one will list only directories -
        much like ``ls -d */`` command does.

        For the following bucket::

            dir1/file1
            dir1/file2
            dir2/file3
            dir2/file4
            file5

        it will yield ``dir1/`` and ``dir2/`` (with the trailing slash).

        :param str prefix:
            same as the same-named argument to :meth:`list`.
            Will only yield directories residing under the given path.
        """
        raise NotImplementedError

    def __iter__(self):
        return self.list()

    def _normalize(self, key: str) -> str:
        # cut off any leading slashes
        key = key.lstrip('/')
        # compress any runs of slashes
        key = '/'.join(p for p in key.split('/') if p)
        return key

    def exists(self, key: str) -> bool:
        """
        Check if given ``key`` exists in this bucket.
        Only "files" are considered, not "directories".
        """
        return self._exists(self._normalize(key))

    def _exists(self, key: str):
        raise NotImplementedError

    def download(self, key: str, path: str):
        """
        Download object identified by ``key`` to given local ``path``

        :param str key: object's identifier, may contain slashes
        :param str path: local path to save the file to,
            either directory with trailing ``/`` or full filename.

            If this value has a trailing slash,
            it will be interpreted as a directory.
            In such case `key`'s path (if any) will be ignored
            and only filename will be used for downloaded file.

            If this value doesn't end with a slash
            then it is treated as a final location of the file,
            be it absolute or relative path.
        """
        if path.endswith(op.sep):
            path += op.basename(key)

        base = op.dirname(path)
        if not op.isdir(base):
            raise ValueError('Not exists or not a directory: {}'.format(base))

        return self._download(self._normalize(key), path)

    def _download(self, key: str, fullpath: str):
        raise NotImplementedError

    def fetch(self, key: str, binary: bool = False):
        """
        Obtain given object and return it as a string/blob
        (depending on ``binary`` argument's value).

        On python2 will return `python2:str` for binary
        and `python2:unicode` for text file;
        on python3 will use `py3:bytes` for binary and `py3:str` for text.
        """
        with self.open_r(key, binary) as f:
            return f.read()

    def open_r(self, key: str, binary: bool = False) -> Union[BinaryIO, TextIO]:
        """
        Open given key as a read-only file-like object.
        It should be :meth:`~io.IOBase.close`\ d after usage.
        Can also be used in a ``with`` statement.
        """
        return self._open_r(self._normalize(key), binary)

    def _open_r(self, key: str, binary: bool):
        raise NotImplementedError

    def upload(self, localobj: Union[str, BinaryIO], name: str = None, overwrite: bool = False):
        """
        Upload given local file or directory to the bucket,
        optionally under a different name.
        Also can accept file-like objects (in binary mode).

        :param localobj: it should be one of the following:

            * absolute or relative path to a local file;
            * absolute or relative path to a local directory;
            * file-like object opened in binary mode
              (i.e. which yields `python2:str` in python2
              or `py3:bytes` in python3).

        :param str name: optional target name to be used.
            It is mandatory when ``localobj`` is a file-like object
            and has no sensible ``name`` property.

        :param bool overwrite:
            by default we will refuse to overwrite already-existing object
            with the same name as the one being uploaded.
            This option allows to override that behaviour.

            .. note::
               Overwrite checking requires additional request to the server,
               so it might be useful to disable overwrite check for speed
               when appropriate by passing ``overwrite=True``.
        """
        if hasattr(localobj, 'read'):
            # it is a file-like object, handle it correspondingly
            if hasattr(localobj, 'mode') and 'b' not in localobj.mode:
                # non-binary mode won't work properly for RealS3,
                # especially in py3;
                # that is why we try to figure out such situations early
                raise ValueError('File should be opened in binary mode')

            localname = getattr(localobj, 'name', None)
            if not isinstance(localname, str) or localname == '<fdopen>':
                # for unnamed temporary files, name is either integer fd (py3)
                # or string <fdopen> (py2)
                localname = None  # in case we didn't detect all usecases
                if not name or name.endswith('/'):
                    # if name is not provided or is a directory
                    # then we need filename
                    raise ValueError('File object %r has no name attr, '
                                     'please provide name explicitly', localobj)
        else:
            # ensure no trailing slash
            localobj = localobj.rstrip(op.sep)
            if not op.exists(localobj):
                raise ValueError(
                    'No such file or directory: {}'.format(localobj))
            localname = localobj

        if not name:
            name = op.basename(localname)
        elif name.endswith(op.sep):
            name = op.join(name, op.basename(localname))
        name = self._normalize(name)

        if hasattr(localobj, 'read'):
            return self._upload_fileobj(localobj, name, overwrite)
        return self._upload(localobj, name, overwrite)

    def upload_with_args(self, localobj: Union[str, BinaryIO], name: str = None, overwrite: bool = False, **kwargs):
        """
        Upload given local file or directory to the bucket,
        optionally under a different name.
        Also can accept file-like objects (in binary mode).

        :param localobj: it should be one of the following:

            * absolute or relative path to a local file;
            * absolute or relative path to a local directory;
            * file-like object opened in binary mode
              (i.e. which yields `python2:str` in python2
              or `py3:bytes` in python3).

        :param str name: optional target name to be used.
            It is mandatory when ``localobj`` is a file-like object
            and has no sensible ``name`` property.

        :param bool overwrite:
            by default we will refuse to overwrite already-existing object
            with the same name as the one being uploaded.
            This option allows to override that behaviour.

            .. note::
               Overwrite checking requires additional request to the server,
               so it might be useful to disable overwrite check for speed
               when appropriate by passing ``overwrite=True``.
        """
        if hasattr(localobj, 'read'):
            # it is a file-like object, handle it correspondingly
            if hasattr(localobj, 'mode') and 'b' not in localobj.mode:
                # non-binary mode won't work properly for RealS3,
                # especially in py3;
                # that is why we try to figure out such situations early
                raise ValueError('File should be opened in binary mode')

            localname = getattr(localobj, 'name', None)
            if not isinstance(localname, str) or localname == '<fdopen>':
                # for unnamed temporary files, name is either integer fd (py3)
                # or string <fdopen> (py2)
                localname = None  # in case we didn't detect all usecases
                if not name or name.endswith('/'):
                    # if name is not provided or is a directory
                    # then we need filename
                    raise ValueError('File object %r has no name attr, '
                                     'please provide name explicitly', localobj)
        else:
            # ensure no trailing slash
            localobj = localobj.rstrip(op.sep)
            if not op.exists(localobj):
                raise ValueError(
                    'No such file or directory: {}'.format(localobj))
            localname = localobj

        if not name:
            name = op.basename(localname)
        elif name.endswith(op.sep):
            name = op.join(name, op.basename(localname))
        name = self._normalize(name)

        if hasattr(localobj, 'read'):
            return self._upload_fileobj_with_args(localobj, name, overwrite, **kwargs)
        return self._upload_with_args(localobj, name, overwrite, **kwargs)

    def _upload(self, localobj: str, name: str = None, overwrite: bool = False):
        raise NotImplementedError

    def _upload_with_args(self, localobj: str, name: str = None, overwrite: bool = False, **kwargs):
        raise NotImplementedError

    def _upload_fileobj(self, localobj: Union[TextIO, BinaryIO], name: str = None, overwrite: bool = False):
        raise NotImplementedError

    def _upload_fileobj_with_args(self, localobj: Union[TextIO, BinaryIO], name: str = None, overwrite: bool = False, **kwargs):
        raise NotImplementedError

    def remove(self, key: str, recursive: bool = False):
        """
        Delete an object denoted by given key.

        :param str key: either full name of the object to handle
            or a common prefix ("directory name").
        :param bool recursive: by default we won't delete directories.
            This argument should be set to `True` to allow that.
        """
        return self._remove(self._normalize(key), recursive)

    def _remove(self, key: str, recursive: bool):
        raise NotImplementedError

    def rename(self, src: str, dst: str):
        """
        Rename object in-place, without downloading or uploading.
        This renaming is done within the same bucket only.

        :param str src: key of an existing object (not directory)
        :param str dst: target object name.
        """
        return self._rename(self._normalize(src), self._normalize(dst))

    def _rename(self, src: str, dst: str):
        raise NotImplementedError

    def copy_from(self, key: str, from_bucket: Union[str, BaseS3Helper] = None, from_key: str = None):
        """
        Copy object from given bucket&key to this bucket.
        This method should usually be preferred over :meth:`copy_to`
        because it does not create `S3Helper` instance for destination bucket.
        An exception is when you want to use custom boto session
        with STS credentials.

        :param str key: target key
        :param from_bucket: (`str` or `BaseS3Helper`) --
            reference to the source bucket to copy from;
            defaults to this bucket if omitted.
        :param str from_key: source object's key;
            defaults to the same key as target one.
        """
        if not from_bucket and not from_key:
            raise ValueError(
                'At least one of (from_bucket, from_key) should be provided')

        if not from_bucket:
            from_bucket = self.bucket
        if isinstance(from_bucket, BaseS3Helper):
            # FIXME here we unwrap bucket name
            # but later in LocalS3 we wrap it again...
            from_bucket = from_bucket.bucket
        elif not from_key:
            from_key = key

        return self._copy_from(
            from_bucket,
            self._normalize(from_key),
            self._normalize(key),
        )

    def copy_from_with_args(self, key: str, from_bucket: Union[str, BaseS3Helper] = None, from_key: str = None,
                            **kwargs):
        """
        Copy object from given bucket&key to this bucket with additional key word arguments.
        This method should usually be preferred over :meth:`copy_to`
        because it does not create `S3Helper` instance for destination bucket.
        An exception is when you want to use custom boto session
        with STS credentials.

        :param str key: target key
        :param from_bucket: (`str` or `BaseS3Helper`) --
            reference to the source bucket to copy from;
            defaults to this bucket if omitted.
        :param str from_key: source object's key;
            defaults to the same key as target one.
        """
        if not from_bucket and not from_key:
            raise ValueError(
                'At least one of (from_bucket, from_key) should be provided')

        if not from_bucket:
            from_bucket = self.bucket
        if isinstance(from_bucket, BaseS3Helper):
            # FIXME here we unwrap bucket name
            # but later in LocalS3 we wrap it again...
            from_bucket = from_bucket.bucket
        elif not from_key:
            from_key = key

        return self._copy_from_with_args(
            from_bucket,
            self._normalize(from_key),
            self._normalize(key),
            **kwargs
        )

    def _copy_from(self, from_bucket: Union[str, BaseS3Helper], from_key: str, to_key: str):
        raise NotImplementedError

    def _copy_from_with_args(self, from_bucket: str, from_key: str, to_key: str, **kwargs):
        raise NotImplementedError

    def copy_to(self, key: str, to_bucket: Union[str, BaseS3Helper] = None, to_key: str = None):
        """
        Copy object from this bucket to a different bucket and key.

        Usually you should prefer :meth:`copy_from`
        unless you need to use custom boto session which was
        set for the *source* bucket.

        Internally it will issue :meth:`copy_from` for the ``to_bucket``
        using the same boto session as the one used for this object,
        which is useful when copying stuff between AWS accounts.

        :param str key: key of an existing object (not directory)
        :param to_bucket: either name/path of the dest bucket
            or a `BaseS3Helper` subclass denoting target bucket.
            Defaults to current bucket.
            Should resolve to the same type (`LocalS3`, `RealS3`)
            as the current class.
        :param str to_key: key under which to write the objcet
            on target bucket.
            Defaults to original key.
        """
        if not to_bucket and not to_key:
            raise ValueError(
                'At least one of (to_bucket, to_key) should be provided')

        if to_bucket:
            if not isinstance(to_bucket, BaseS3Helper):
                # string name? wrap it with our class
                to_bucket = self._create_similar(bucket=to_bucket)
            elif not isinstance(to_bucket, type(self)):
                # ensure we are not trying to copy from local to real etc
                raise ValueError('Cannot copy between {} and {}'.format(
                    type(self),
                    type(to_bucket),
                ))

        return self._copy_to(
            self._normalize(key),
            to_bucket or self,
            self._normalize(to_key or key),
        )

    def copy_to_with_args(self, key: str, to_bucket: Union[str, BaseS3Helper] = None, to_key: str = None, **kwargs):
        """
        Copy object from this bucket to a different bucket and key with additional key word arguments.

        Usually you should prefer :meth:`copy_from`
        unless you need to use custom boto session which was
        set for the *source* bucket.

        Internally it will issue :meth:`copy_from` for the ``to_bucket``
        using the same boto session as the one used for this object,
        which is useful when copying stuff between AWS accounts.

        :param str key: key of an existing object (not directory)
        :param to_bucket: either name/path of the dest bucket
            or a `BaseS3Helper` subclass denoting target bucket.
            Defaults to current bucket.
            Should resolve to the same type (`LocalS3`, `RealS3`)
            as the current class.
        :param str to_key: key under which to write the objcet
            on target bucket.
            Defaults to original key.
        """
        if not to_bucket and not to_key:
            raise ValueError(
                'At least one of (to_bucket, to_key) should be provided')

        if to_bucket:
            if not isinstance(to_bucket, BaseS3Helper):
                # string name? wrap it with our class
                to_bucket = self._create_similar(bucket=to_bucket)
            elif not isinstance(to_bucket, type(self)):
                # ensure we are not trying to copy from local to real etc
                raise ValueError('Cannot copy between {} and {}'.format(
                    type(self),
                    type(to_bucket),
                ))

        return self._copy_to_with_args(
            self._normalize(key),
            to_bucket or self,
            self._normalize(to_key or key),
            **kwargs
        )

    def _copy_to(self, src_key: str, dst_bucket: BaseS3Helper, dst_key: str):
        raise NotImplementedError

    def _copy_to_with_args(self, src_key: str, dst_bucket: BaseS3Helper, dst_key: str, **kwargs):
        raise NotImplementedError

    def _create_similar(self, bucket: str):
        """
        Internal method:
        create an instance of our type but for the different bucket.
        For RealS3 it will use the same session as the one used for self,
        thus retaining the same STS credentials
        (unlike just `type(self)(bucket)` which uses default account).
        """
        return type(self)(bucket)

    def __repr__(self):
        return '<{cname} bucket={bucket}>'.format(
            cname=self.__class__.__name__,
            bucket=self.bucket,
        )


