from __future__ import annotations
import io
import os
from os import path as op
from typing import Generator, TextIO, BinaryIO, Union

import boto3
from botocore.exceptions import ClientError

from kasasa_common.aws.s3.base_s3 import BaseS3Helper, BucketNotAvailable, ResponseWrapper
from kasasa_common.util.file_system_helpers import makedirs_unless_exist
from kasasa_common import logger


class RealS3(BaseS3Helper):
    """
    Implementation of S3 helper using real AWS S3 service.
    Can also work with `moto <https://github.com/spulec/moto>`_.
    """

    # extra parameters to pass to S3 when uploading files;
    # also used when copying.
    upload_extra_args = {
        'ServerSideEncryption': 'AES256',
    }

    def __init__(self, bucket, session=None, validate=True):
        """
        :param session: optional Boto3 session to use;
            can be obtained by :meth:`sts.get_sts_session`.
        :param bool validate: whether we want to check that such bucket exists.
            Can be set to `False` for optimization
            when one needs to create multiple helper instances
            for the same bucket which is known to exist.
        """
        super(RealS3, self).__init__(bucket)

        self.session = session
        self.S3 = (session or boto3).resource('s3')

        # check that bucket exists
        # there is no high-level way to check that, so use low-level client
        # https://stackoverflow.com/a/26871885/2267932
        try:
            if validate:
                self.S3.meta.client.head_bucket(Bucket=bucket)
        except ClientError as e:
            if e.response.get('Error', {}).get('Code') != '404':
                raise
            raise BucketNotAvailable(
                'Bucket does not exist or is not available: {}'.format(bucket))
        self.bucket_obj = self.S3.Bucket(bucket)

    def list(self, prefix: str = None, only_after: str = None) -> Generator:
        objects = self.bucket_obj.objects.all()
        if prefix:
            objects = objects.filter(
                # Important: we should end with a slash,
                # or else 'dir2' prefix will also list 'dir254/file.txt'
                Prefix=prefix.strip('/') + '/',
            )

        if only_after:
            # here we can just pass-through
            objects = objects.filter(
                Marker=only_after,
            )

        for obj in objects:
            yield obj.key

    def list_prefix(self, prefix: str) -> Generator:
        for obj in self.bucket_obj.objects.filter(Prefix=prefix):
            logger.info('list_prefix %s', obj.key)
            yield obj.key

    def list_dirs(self, prefix: str = None) -> Generator:
        if prefix:
            prefix = prefix.strip('/')
        else:
            prefix = ''  # None cannot be supported directly
        if prefix:
            prefix += '/'  # but for '' don't add it, and '/' gets ''

        # boto3 does not expose CommonPrefixes via "resource" API,
        # so we use underlying client directly.
        # https://stackoverflow.com/a/32674165/2267932
        paginator = self.bucket_obj.meta.client.get_paginator('list_objects')
        for result in paginator.paginate(
                Bucket=self.bucket,
                Delimiter='/',
                Prefix=prefix,
        ):
            for compref in result.get('CommonPrefixes', []):
                yield compref['Prefix']

    def _exists(self, key: str) -> bool:
        # We don't want to download object's body,
        # so we use the same trick as in __init__:
        # call `client.head_object()` and check if it raises an exception.
        try:
            self.S3.meta.client.head_object(Bucket=self.bucket, Key=key)
            return True
        except ClientError as e:
            if e.response.get('Error', {}).get('Code') != '404':
                # TODO maybe handle something like "access denied"
                # as "existing" object?
                raise
            # 404 means object does not exist
            return False

    def _download(self, key: str, fullpath: str):
        # Important notice: unlike local filesystem,
        # S3 may have both path/to/file.txt and path/to files.
        # In such case we will do the following:
        # - if the key we were given ends with a slash then we fetch directory;
        # - else we fetch a file.

        # Looks like s3 first yields the shortest key first,
        # but we cannot be sure
        try:
            # let's pretend it is a file
            # (which is hopefully not the case if key ends with a slash)
            self.bucket_obj.download_file(key, fullpath)
        except ClientError:
            # it is probably not a file but directory (or nothing),
            # let's try to download it as a directory.
            # first, we don't want to fetch longdir/ contents
            # if we need `long` directory. So let's add a separator.
            if not key.endswith('/'):
                key += '/'
            cutchars = len(key)
            got = False
            for obj in self.bucket_obj.objects.filter(Prefix=key):
                got = True
                # obj is ObjectSummary instance
                targetpath = op.join(fullpath, obj.key[cutchars:])
                makedirs_unless_exist(op.dirname(targetpath))
                obj.Object().download_file(targetpath)
            if not got:
                raise ValueError('Key {} is neither file nor directory'.format(
                    key))

    def _open_r(self, key: str, binary: str) -> Union[BinaryIO, TextIO]:
        obj = self.bucket_obj.Object(key)
        ret = obj.get()
        body = ret['Body']
        # Originally, body returned is boto's wrapper
        # around urllib3's response.
        # Urllib3's one is compatible with io module and with stmt.
        # while the wrapper isn't.
        # The reason for that wrapper is to support timeouts
        # and check for incomplete reads;
        # we don't use timeouts and incomplete reads check is now implemented
        # in urllib3's version itself.
        body = body._raw_stream
        body.enforce_content_length = True  # default is false

        # now apply wrappers
        body = ResponseWrapper(body)  # this one is temporary, see docstring
        body = io.BufferedReader(body)
        if binary:
            return body  # use it as is
        return io.TextIOWrapper(body)

    def _upload_check_overwrite(self, fullpath):
        for obj in self.bucket_obj.objects.filter(Prefix=fullpath):
            # for fullpath='path/to', accept 'path/to' and 'path/to/file'
            # but not 'path/toast'
            if obj.key == fullpath or \
                    obj.key[len(fullpath):].startswith('/'):
                raise ValueError('Won\'t overwrite existing object')

    def _upload(self, localobj: str, name: str = None, overwrite: bool = False):
        if not overwrite:
            self._upload_check_overwrite(name)

        if op.isfile(localobj):
            self.bucket_obj.upload_file(
                localobj, name, ExtraArgs=self.upload_extra_args)
        elif op.isdir(localobj):
            for sub in os.listdir(localobj):
                self._upload(
                    op.join(localobj, sub),
                    op.join(name, sub),
                    overwrite=True,  # as we should have already checked that
                )

    def _upload_with_args(self, localobj: str, name: str = None, overwrite: bool = False, **kwargs):
        if not overwrite:
            self._upload_check_overwrite(name)

        if op.isfile(localobj):
            all_args = self.upload_extra_args.copy()
            all_args.update(dict(**kwargs))
            self.bucket_obj.upload_file(
                localobj, name, ExtraArgs=all_args)
        elif op.isdir(localobj):
            for sub in os.listdir(localobj):
                self._upload_with_args(
                    op.join(localobj, sub),
                    op.join(name, sub),
                    overwrite=True,  # as we should have already checked that
                    **kwargs
                )

    def _upload_fileobj(self, localobj: Union[TextIO, BinaryIO], name: str = None, overwrite: bool = False):
        if not overwrite:
            self._upload_check_overwrite(name)

        self.bucket_obj.upload_fileobj(
            localobj, name, ExtraArgs=self.upload_extra_args)

    def _upload_fileobj_with_args(self, localobj: Union[TextIO, BinaryIO], name: str = None, overwrite: bool = False,
                                  **kwargs):
        if not overwrite:
            self._upload_check_overwrite(name)

        all_args = self.upload_extra_args.copy()
        all_args.update(dict(**kwargs))

        self.bucket_obj.upload_fileobj(
            localobj, name, ExtraArgs=all_args)

    def _remove(self, key: str, recursive: bool):
        if recursive:
            raise NotImplementedError(
                'Recursive deleting is not implemented yet')
        else:
            self.bucket_obj.Object(key).delete()
            # TODO check result

    def _rename(self, src: str, dst: str):
        # S3 has no rename API, so we will have to copy&delete:
        o = self.bucket_obj.Object(dst)
        # copy means "copy from whatever location to this object".
        # Unlike `copy_from` which is basically limited to 5GB,
        # this `copy` will use multiple threads if required
        # and its size limit is 5TB.
        # See https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html
        o.copy(
            {'Bucket': self.bucket, 'Key': src},
            # as we are actually writing new object,
            # we will want to use upload args here as well.
            ExtraArgs=self.upload_extra_args,
        )
        # and now delete original file
        self.remove(src)

    def _copy_from(self, from_bucket: Union[str, RealS3], from_key: str, to_key: str):
        dst_obj = self.bucket_obj.Object(to_key)
        # copy means "copy from whatever location to this object"
        dst_obj.copy(
            {'Bucket': from_bucket, 'Key': from_key},
            ExtraArgs=self.upload_extra_args,
        )

    def _copy_from_with_args(self, from_bucket: Union[str, RealS3], from_key: str, to_key: str, **kwargs):
        dst_obj = self.bucket_obj.Object(to_key)
        # copy means "copy from whatever location to this object"
        all_args = self.upload_extra_args.copy()
        all_args.update(dict(**kwargs))
        dst_obj.copy(
            {'Bucket': from_bucket, 'Key': from_key},
            ExtraArgs=all_args,
        )

    def _copy_to(self, src_key: str, dst_bucket: BaseS3Helper, dst_key: str):
        return dst_bucket._copy_from(self.bucket, src_key, dst_key)

    def _copy_to_with_args(self, src_key: str, dst_bucket: BaseS3Helper, dst_key: str, **kwargs):
        return dst_bucket._copy_from_with_args(self.bucket, src_key, dst_key, **kwargs)

    def _create_similar(self, bucket: str):
        """
        Internal method:
        create an instance of our type but for the different bucket.
        For RealS3 it will use the same session as the one used for self,
        thus retaining the same STS credentials
        (unlike just `type(self)(bucket)` which uses default account).
        """
        # XXX we don't validate bucket name here
        # to speed things up.
        return RealS3(bucket, session=self.session, validate=False)