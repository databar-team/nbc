import threading
import queue
from time import sleep
from typing import Tuple, Callable, Union, List

from kasasa_common.concurrency.event_manager import EventManager
from kasasa_common.queue import get_queue_interface
from kasasa_common.context.context import ProducerConsumerContext
from kasasa_common.logger import logger


class ProducerConsumer:
    """
    ProducerConsumer is a mechanism for asynchronously loading work onto a queue, via the producer,
    while a consumer drains that queue. This is a handy method of loading up work locally which is spawned from
    some synchronous process. i.e. work is being drawn from an SQS via the producer while the consumer is processing
    that work. This decouples the processing of work from the wait of a return from a separate tool.

    NOTE: If network latency is not a concern for the generation of any work, a ThreadSafeGenerator shared between
    threads may bet a better option. Use this if network latency is a concern for one or more pattern of work production

    Example Usage:
        >>> from kasasa_common.concurrency import ProducerConsumer
        >>> pc = ProducerConsumer()
        >>> result_list = []
        >>> def consumer_func(payload):
        ...     result_list.append(payload)
        >>> def producer_func(q, item):
        ...     q.put(dict(job_type="demo", payload=item))
        >>> pc.register_producer(name="test_producer", producer_func=producer_func,
        ...                      producer_args=(True,))
        >>> pc.register_consumer(name="demo", consumer_func=consumer_func)
        >>> pc.run()
        [2020-04-20 14:04:36,277] INFO     in producer_consumer:188: Finished test_producer
        [2020-04-20 14:04:36,277] INFO     in producer_consumer:228: Finished consumer_0
        [2020-04-20 14:04:36,277] INFO     in producer_consumer:167: completed work, 0 exceptions were encountered.
        >>> print(result_list)
        [True]
    """
    def __init__(self, context: ProducerConsumerContext = ProducerConsumerContext()):
        """

        :param context: Context object with defined configurations for ProducerConsumer and it's work queue
        """
        self.event_manager = EventManager()
        self._work_queue_class = get_queue_interface(context.QUEUE_TYPE)
        if self._work_queue_class is None:
            raise ValueError("Received invalid queue_type")
        self._work_queue = self._work_queue_class(**context.QUEUE_KWARGS)

        self.max_consumers = context.MAX_CONSUMERS

        self._producers = dict()
        self._consumers = dict()
        self.threads: List[threading.Thread] = []
        self._thread_list_lock = threading.Lock()
        self.thread_exception_list = []
        self._exception_list_lock = threading.Lock()

    def register_producer(self, name: str, producer_func: Callable, producer_args: Tuple = (),
                          producer_kwargs: dict = dict()):
        """
        Registers a producer that will be used to generate work for the consumers.

        Example producer_func signature:
            def sqs_producer(work_queue, *args, **kwargs) -> None

        A valid producer function will accept a work_queue, of type queue.Queue, and perform put
        operations to add work to it.

        Example payload for work_queue.put():
            dict(
                job_type="consumer_func_1",  # this is the name that is registered for a consumer function
                payload=Any                  # Any value as long as it is assigned to the payload key
            )

        :param name: str value used to represent the key for this producer function
        :param producer_func: callable function that adds work to the queue
        :param producer_args: tuple of config related arguments that will be passed to the producer
        :param producer_kwargs: dictionary of config related kwargs that will be passed to the producer
        :return:
        """
        self._producers[name] = dict(
            func=producer_func,
            args=producer_args,
            kwargs=producer_kwargs
        )

    def unregister_producer(self, name: str) -> Union[dict, None]:
        """
        Takes in a name and unregisters it from the producers and returns that producer and config

        :param name: name of the producer to unregister
        :return:
        """
        return self._producers.pop(name, None)

    def register_consumer(self, name: str, consumer_func: Callable, consumer_args: Tuple = (),
                          consumer_kwargs: dict = dict()):
        """
        Registers a consumer consume work from the queue.

        Example producer_func signature:
            def sqs_consumer(payload, *args, **kwargs) -> None

        A valid consumer function will accept a payload of the type given by the producer and perform
        operations on it. The payload passed to this function will match whatever was added under the
        payload key by the producer.

        :param name: str value used to represent the key for this consumer function. This should match the `job_type`
                    key that is provided by the producer in the dictionary payload to the work_queue
        :param consumer_func: callable function that processes a payload
        :param consumer_args: tuple of config related arguments that will be passed to the consumer
        :param consumer_kwargs: dictionary of config related kwargs that will be passed to the consumer
        :return:
        """
        self._consumers[name] = dict(
            func=consumer_func,
            args=consumer_args,
            kwargs=consumer_kwargs
        )

    def unregister_consumer(self, name: str) -> Union[dict, None]:
        """
        Takes in a name and unregisters it from the consumers and returns that consumer and config

        :param name: name of the consumer to unregister
        :return:
        """
        return self._consumers.pop(name, None)

    def start_producer(self, name: str) -> (bool, Union[str, None]):
        """
        Starts the given producer. This gives the ability to spawn a thread for a producer that is added while jobs
        are already running.

        :param name: str name of the registered producer
        :return (result, error): bool value of result of attempt and error message if applicable or None if not
        """
        with self._thread_list_lock:
            if len(self.threads) < 1:
                return False, "No threads currently running. Use run method first."
        producer_map = self._producers.get(name, None)
        if producer_map is None:
            return False, f"No registered producer named {name}"
        event = self.event_manager.events.get(name, None)
        if event is not None:
            if not event.is_set():
                return False, f"Producer with name {name} is already running"
        self.event_manager.add_event(name)
        thread = threading.Thread(
            name=name,
            target=self._producer,
            args=(producer_map["func"], producer_map["args"], producer_map["kwargs"])
        )
        self.threads.append(thread)
        thread.start()
        return True, None

    def run(self):
        """
        `run` spawns a thread per producer that is registered and threads matching the max_consumers config value.

        The producer threads are spawned and passed the configuring args/kwargs during thread registration.

        The consumer threads are spawned and passed no args/kwargs. These are picked up from the self.consumers
        dictionary by the self._consumer method as it can't be predicted what work will need to be done.

        `run` will wait until all threads have completed their work before finally closing out by logging the number
        of unhandled errors that were raised.

        :return:
        """
        for name in self._producers.keys():
            self.start_producer(name)

        for num in range(0, self.max_consumers):
            thread = threading.Thread(
                name=f"consumer_{num}",
                target=self._consumer
            )
            self.threads.append(thread)
            thread.start()

        for t in self.threads:
            t.join()

        if not self._work_queue.empty:
            logger.error(f"Work Queue was not empty when it should be for producer list {self._producers.keys}")
        logger.info(f"completed work, {len(self.thread_exception_list)} exceptions were encountered.")

    def _producer(self, producer_func, producer_args, producer_kwargs):
        """
        _producer takes a function that receives a queue object as the first argument and any number of args or kwargs.
        The provided function populates the queue with work that will be consumed by the consumer_func asynchronously.
        _producer will wait until the producer_func returns and them close out the thread and notify the event manager
        of completion of the work. If the producer function fails to handle an exception, the thread will be killed and
        no further work will be processed by the producer.
        """
        try:
            # completes when the source the function is pulling from is empty
            producer_func(self._work_queue, *producer_args, **producer_kwargs)
        except Exception as e:
            name = threading.current_thread().getName()
            logger.error(f"Thread {name} encountered an unhandled exception in producer function: {e}",
                         exc_info=True)
            with self._exception_list_lock:
                self.thread_exception_list.append(dict(name=name, err=e))

        # notify consuming threads we are done
        logger.info(f"Finished {threading.current_thread().getName()}")
        self.event_manager.set(threading.current_thread().getName())

    def _consumer(self):
        """
        _consumer reads the work queue and spawns work based on the job_type argument received from the queue. The
        self._consumers keys are matched against job type to determine which consumer function should be used
        for the consumption of the attached payload.

        If the consumer function raises an unhandled exception, _consumer will log it and move on to the next item
        in the queue. This happens until the queue is drained and the event_manager has been notified by all producers
        that there is no more work to be processed.

        :return:
        """
        while not self._work_queue.empty or not self.event_manager.is_set():
            try:
                job = self._work_queue.get()
                func = self._consumers.get(job["job_type"])

                func["func"](job["payload"], *func['args'], **func['kwargs'])

                self._work_queue.task_done()
            except queue.Empty:
                # This handles a race condition between threads where the queue became empty after entering
                # the current loop iteration.
                if self.event_manager.is_set():
                    # if nothing in queue and producer signalled we are done,
                    # then exit out of loop to stop thread
                    break
                sleep(1)
            except Exception as e:
                name = threading.current_thread().getName()
                logger.error(f"Thread {name} encountered an unhandled exception on consumer function: {e}",
                             exc_info=True)
                with self._exception_list_lock:
                    self.thread_exception_list.append(dict(name=name, err=e))
                self._work_queue.task_done()
                continue  # move on with processing the queue

        logger.info(f"Finished {threading.current_thread().getName()}")
